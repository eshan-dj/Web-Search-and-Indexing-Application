{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f597406-21bc-4a2a-8d5a-15ddda098d77",
   "metadata": {},
   "source": [
    "## Merged Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18d0e7be-6903-4835-9377-e4edb533a31e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final inverted index saved successfully!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def load_index(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "        return {item.split(':')[0].strip(): [int(x) for x in item.split(':')[1].strip().split(',')] for item in data}\n",
    "        \n",
    "\n",
    "expedia_index = load_index('output_expedia.json')\n",
    "airbnb_index = load_index('output_airbnb.json')\n",
    "lonelyplanet_index = load_index('output_lonelyplanet.json')\n",
    "\n",
    "final_index = {}\n",
    "\n",
    "for index in [expedia_index, airbnb_index, lonelyplanet_index]:\n",
    "    for term, doc_ids in index.items(): \n",
    "        if term in final_index:\n",
    "            final_index[term].extend(doc_ids)  \n",
    "        else:\n",
    "            final_index[term] = doc_ids\n",
    "\n",
    "\n",
    "with open('final_inverted_index.json', 'w') as f:\n",
    "    json.dump(final_index, f, indent=4)\n",
    "\n",
    "print(\"Final inverted index saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb76f8d-9dee-4edb-a8cb-cc06b958b865",
   "metadata": {},
   "source": [
    "## TF-IDF Calculation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "461b7594-f8ee-4b78-8f5c-c6c58d69eeb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF document vectors saved!\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from collections import defaultdict\n",
    "\n",
    "with open('final_inverted_index.json', 'r') as f:\n",
    "    inverted_index = json.load(f)\n",
    "\n",
    "doc_ids = set()\n",
    "for term_docs in inverted_index.values():\n",
    "    doc_ids.update(term_docs)\n",
    "N = len(doc_ids)\n",
    "\n",
    "# TF-IDF scores\n",
    "tf_idf = defaultdict(dict)\n",
    "\n",
    "for term, docs in inverted_index.items():\n",
    "    df = len(docs)  \n",
    "    idf = math.log((N / df) + 1) \n",
    "\n",
    "    for doc in docs:\n",
    "        tf = docs.count(doc) / len(docs)  \n",
    "        tf_idf[doc][term] = tf * idf \n",
    "\n",
    "with open('tf_idf_vectors.json', 'w') as f:\n",
    "    json.dump(tf_idf, f, indent=4)\n",
    "\n",
    "print(\"TF-IDF document vectors saved!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e7e7191-67d2-48b9-9945-06c1ed0dea22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_query(query, inverted_index, N):\n",
    "    query_terms = query.lower().split()\n",
    "    query_vector = {}\n",
    "\n",
    "    for term in query_terms:\n",
    "        if term in inverted_index:\n",
    "            df = len(inverted_index[term])  \n",
    "            idf = math.log((N / df) + 1) \n",
    "            query_vector[term] = idf  \n",
    "\n",
    "    return query_vector\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e31de13-4755-43bd-9351-3029d199bf62",
   "metadata": {},
   "source": [
    "## Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "974b5c48-ba76-4b59-a0df-3d1efe82549e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ranked Documents: [('12', np.float64(0.00021951006188713817)), ('17', np.float64(0.00013581062605648786)), ('9', np.float64(0.000102494491456348)), ('8', np.float64(9.172958825055138e-05)), ('11', np.float64(8.942504437114426e-05)), ('6', np.float64(6.15341330602662e-05)), ('5', np.float64(6.117056241381001e-05)), ('4', np.float64(5.551387842034257e-05)), ('1', np.float64(5.010037011795599e-05)), ('0', np.float64(4.895969656409375e-05)), ('48', np.float64(4.789109523589405e-05)), ('16', np.float64(4.722463736632753e-05)), ('3', np.float64(4.661408262834144e-05)), ('10', np.float64(3.475076568209261e-05)), ('2', np.float64(3.13313892166242e-05)), ('44', np.float64(2.7441654191507667e-05)), ('25', np.float64(1.735590747269721e-05)), ('26', np.float64(1.735590747269721e-05)), ('27', np.float64(1.735590747269721e-05)), ('30', np.float64(1.735590747269721e-05)), ('32', np.float64(1.735590747269721e-05)), ('34', np.float64(1.735590747269721e-05)), ('35', np.float64(1.735590747269721e-05)), ('37', np.float64(1.735590747269721e-05)), ('39', np.float64(1.735590747269721e-05)), ('41', np.float64(1.735590747269721e-05)), ('43', np.float64(1.735590747269721e-05)), ('22', np.float64(1.7340558990963157e-05)), ('23', np.float64(1.7340558990963157e-05)), ('29', np.float64(1.7340558990963157e-05)), ('31', np.float64(1.7340558990963157e-05)), ('33', np.float64(1.7340558990963157e-05)), ('36', np.float64(1.7340558990963157e-05)), ('38', np.float64(1.7340558990963157e-05)), ('40', np.float64(1.7340558990963157e-05)), ('42', np.float64(1.7340558990963157e-05)), ('7', np.float64(1.6125252960690713e-05)), ('50', np.float64(1.4374200948307973e-05)), ('55', np.float64(1.4374200948307973e-05)), ('52', np.float64(1.435780288855726e-05)), ('54', np.float64(1.435780288855726e-05)), ('51', np.float64(1.4256685363563803e-05)), ('53', np.float64(1.4256685363563803e-05)), ('62', np.float64(1.0750158486466071e-05)), ('64', np.float64(1.0627575712050198e-05)), ('65', np.float64(1.0627575712050198e-05)), ('58', np.float64(1.0584264275853733e-05)), ('20', np.float64(9.696625432285048e-06)), ('45', np.float64(9.660618448890956e-06)), ('67', np.float64(7.74913319599081e-06)), ('46', np.float64(7.218924291329169e-06)), ('84', np.float64(6.664896200108235e-06)), ('68', np.float64(6.321041645005565e-06)), ('19', np.float64(5.969283089093776e-06)), ('61', np.float64(5.792073645366006e-06)), ('76', np.float64(5.567161221431966e-06)), ('75', np.float64(5.484566038293674e-06)), ('74', np.float64(4.3841845285429745e-06)), ('70', np.float64(4.329072312799761e-06)), ('80', np.float64(4.230516059135432e-06)), ('83', np.float64(4.216124549690696e-06)), ('66', np.float64(4.142355628677198e-06)), ('73', np.float64(4.085430620337737e-06)), ('71', np.float64(4.046715043032661e-06)), ('69', np.float64(3.7354929408456746e-06)), ('81', np.float64(3.6872095354387732e-06)), ('86', np.float64(2.425873153055426e-06)), ('85', np.float64(2.417022374038113e-06)), ('88', np.float64(2.295789617242947e-06)), ('47', np.float64(2.2754997873606084e-06)), ('87', np.float64(2.1768610158522274e-06)), ('49', np.float64(1.1835570768280316e-06)), ('82', np.float64(8.030809058741775e-07)), ('78', np.float64(7.02534036060463e-07)), ('77', np.float64(6.905836096588193e-07)), ('72', np.float64(6.812570350124696e-07)), ('79', np.float64(6.206778672126329e-07)), ('13', 0), ('14', 0), ('15', 0), ('18', 0), ('21', 0), ('24', 0), ('28', 0), ('56', 0), ('57', 0), ('59', 0), ('60', 0), ('63', 0)]\n"
     ]
    }
   ],
   "source": [
    "from numpy.linalg import norm\n",
    "import numpy as np\n",
    "\n",
    "with open('tf_idf_vectors.json', 'r') as f:\n",
    "    doc_vectors = json.load(f)\n",
    "\n",
    "def cosine_similarity(query_vector, doc_vector):\n",
    "    \n",
    "    query_vec = np.array([query_vector.get(term, 0) for term in doc_vector.keys()])\n",
    "    doc_vec = np.array([doc_vector[term] for term in doc_vector.keys()])\n",
    "\n",
    "    if norm(query_vec) == 0 or norm(doc_vec) == 0:\n",
    "        return 0  \n",
    "\n",
    "    return np.dot(query_vec, doc_vec) / (norm(query_vec) * norm(doc_vec))\n",
    "\n",
    "\n",
    "def rank_documents(query):\n",
    "    query_vector = process_query(query, inverted_index, N)\n",
    "    similarities = []\n",
    "\n",
    "    for doc_id, doc_vector in doc_vectors.items():\n",
    "        similarity = cosine_similarity(query_vector, doc_vector)\n",
    "        similarities.append((doc_id, similarity))\n",
    "\n",
    "    ranked_docs = sorted(similarities, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    return ranked_docs\n",
    "\n",
    "# Example query\n",
    "query = \"best travel destinations\"\n",
    "ranked_results = rank_documents(query)\n",
    "print(\"Ranked Documents:\", ranked_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "593a2c0c-a281-4f36-97f2-c2335f07df98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents processed: 89\n",
      "Total terms in collection: 1907426\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from collections import defaultdict\n",
    "import math\n",
    "\n",
    "with open('final_inverted_index.json', 'r') as f:\n",
    "    inverted_index = json.load(f)\n",
    "\n",
    "doc_term_counts = defaultdict(lambda: defaultdict(int))\n",
    "doc_lengths = defaultdict(int)\n",
    "\n",
    "for term, docs in inverted_index.items():\n",
    "    for doc_id in docs:\n",
    "        doc_term_counts[doc_id][term] += 1\n",
    "        doc_lengths[doc_id] += 1\n",
    "\n",
    "total_terms = sum(doc_lengths.values())\n",
    "vocab_size = len(inverted_index)\n",
    "\n",
    "print(\"Documents processed:\", len(doc_term_counts))\n",
    "print(\"Total terms in collection:\", total_terms)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17fd6c2c-a1a0-463b-89dd-942a4bb6270d",
   "metadata": {},
   "source": [
    "## Query Liklihood Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "53796f7c-c95d-43bf-bb26-eed9ddd4b357",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_query_likelihood(query, mu=2000):\n",
    "    query_terms = query.lower().split()\n",
    "    doc_scores = {}\n",
    "\n",
    "    for doc_id, term_counts in doc_term_counts.items():\n",
    "        doc_length = doc_lengths[doc_id]\n",
    "        score = 0\n",
    "\n",
    "        for term in query_terms:\n",
    "            \n",
    "            term_freq = term_counts.get(term, 0)\n",
    "            p_doc = term_freq / doc_length if doc_length > 0 else 0\n",
    "\n",
    "            collection_term_freq = sum(inverted_index.get(term, []))\n",
    "            p_collection = collection_term_freq / total_terms if total_terms > 0 else 1e-10\n",
    "\n",
    "            # Dirichlet smoothing\n",
    "            smoothed_prob = (doc_length / (doc_length + mu)) * p_doc + (mu / (doc_length + mu)) * p_collection\n",
    "            score += math.log(smoothed_prob) if smoothed_prob > 0 else 0\n",
    "\n",
    "        doc_scores[doc_id] = score\n",
    "\n",
    "    ranked_docs = sorted(doc_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    return ranked_docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "82f3e860-cf36-49de-ac2c-b500d1889df2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ranked Documents: [(87, -10.912468893166253), (85, -10.91532196069716), (88, -10.91532196069716), (86, -10.917221747114052), (8, -11.08519641582918), (67, -11.277244568709076), (70, -11.282502582982545), (75, -11.28436045585186), (69, -11.295471595541066), (68, -11.302881503842077)]\n"
     ]
    }
   ],
   "source": [
    "query = \"best travel destinations\"\n",
    "ranked_results = compute_query_likelihood(query)\n",
    "print(\"Ranked Documents:\", ranked_results[:10])  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
